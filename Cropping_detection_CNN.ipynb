{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "from local_package.ConvNN import preproces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (26509, 256, 256)\n",
      "Labels shape: (26509,)\n"
     ]
    }
   ],
   "source": [
    "images = np.load(\"Datasets/images_class_norm.npy\")\n",
    "labels = np.load(\"Datasets/labels_class.npy\")\n",
    "images, labels = shuffle(images, labels, random_state=25140)\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=25140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor\n",
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_images = self.images[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return convert_to_tensor(batch_images), convert_to_tensor(batch_labels)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.images, self.labels = shuffle(self.images, self.labels)\n",
    "\n",
    "train_images_gen = DataGenerator(X_train, y_train, 32)\n",
    "test_images_gen = DataGenerator(X_test, y_test, 32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign custom weights to the first Conv layer\n",
    "def assign_custom_weights_to_Conv2D_layer(model):\n",
    "    conv_layer_with_custom_weights = model.layers[1]\n",
    "    model_weights = conv_layer_with_custom_weights.get_weights()[0]\n",
    "    for x in range(5):\n",
    "        for y in range(5):\n",
    "            model_weights[x,y,0,0] = (4-x)/4\n",
    "            model_weights[y,x,0,1] = (4-x)/4\n",
    "            model_weights[x,y,0,2] = (x)/4\n",
    "            model_weights[y,x,0,3] = (x)/4\n",
    "            # corner gradients\n",
    "            color = -(4-x)*(4-y)/16 if x > 1 or y > 1 else 0\n",
    "            model_weights[x,y,0,4] = color\n",
    "            model_weights[4-y,4-x,0,5] = color\n",
    "            model_weights[4-y,x,0,6] = color\n",
    "            model_weights[y,4-x,0,7] = color\n",
    "            \n",
    "    model.layers[1].set_weights([model_weights, np.zeros(8)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source_image (InputLayer)      [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 8)  208         ['source_image[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 256, 9)  0           ['conv2d[0][0]',                 \n",
      "                                                                  'source_image[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 86, 86, 128)  28928       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 86, 86, 128)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 43, 43, 128)  0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 236672)       0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           3786768     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           544         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,816,481\n",
      "Trainable params: 3,816,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_filters=128\n",
    "dropout_rate=0.6\n",
    "dense_units=32\n",
    "# Input for the source image\n",
    "source_input = Input(shape=(256, 256, 1), name=\"source_image\")\n",
    "\n",
    "# First Conv2D layer\n",
    "x = Conv2D(8, (5, 5), activation=\"relu\", padding=\"same\", strides=(1, 1), use_bias=True)(source_input)\n",
    "\n",
    "# Merge the source image with the output of the first layer\n",
    "merged = Concatenate(axis=-1)([x, source_input])\n",
    "\n",
    "# Second Conv2D layer\n",
    "x = Conv2D(num_filters, (5, 5), activation=\"relu\", padding=\"same\", strides=(3, 3), use_bias=True)(merged)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = MaxPooling2D((2, 2), strides=2, padding='valid')(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(int(dense_units/2), activation=\"relu\", use_bias=True)(x)\n",
    "x = Dense(dense_units, activation=\"relu\", use_bias=True)(x)\n",
    "output = Dense(1, activation=\"sigmoid\", use_bias=True)(x)\n",
    "\n",
    "model = Model(inputs=source_input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "580/580 [==============================] - 87s 111ms/step - loss: 0.6428 - accuracy: 0.6341 - val_loss: 0.6192 - val_accuracy: 0.6722\n",
      "Epoch 2/15\n",
      "580/580 [==============================] - 62s 101ms/step - loss: 0.5810 - accuracy: 0.6928 - val_loss: 0.5569 - val_accuracy: 0.7539\n",
      "Epoch 3/15\n",
      "580/580 [==============================] - 51s 75ms/step - loss: 0.5314 - accuracy: 0.7316 - val_loss: 0.5846 - val_accuracy: 0.6753\n",
      "Epoch 4/15\n",
      "580/580 [==============================] - 48s 78ms/step - loss: 0.3950 - accuracy: 0.8226 - val_loss: 0.3272 - val_accuracy: 0.8927\n",
      "Epoch 5/15\n",
      "580/580 [==============================] - 56s 90ms/step - loss: 0.2672 - accuracy: 0.8924 - val_loss: 0.3155 - val_accuracy: 0.9085\n",
      "Epoch 6/15\n",
      "580/580 [==============================] - 52s 84ms/step - loss: 0.2207 - accuracy: 0.9118 - val_loss: 0.2400 - val_accuracy: 0.9204\n",
      "Epoch 7/15\n",
      "580/580 [==============================] - 50s 81ms/step - loss: 0.1930 - accuracy: 0.9244 - val_loss: 0.2587 - val_accuracy: 0.8956\n",
      "Epoch 8/15\n",
      "580/580 [==============================] - 49s 80ms/step - loss: 0.1777 - accuracy: 0.9316 - val_loss: 0.2125 - val_accuracy: 0.9312\n",
      "Epoch 9/15\n",
      "580/580 [==============================] - 54s 77ms/step - loss: 0.1661 - accuracy: 0.9348 - val_loss: 0.2268 - val_accuracy: 0.9274\n",
      "Epoch 10/15\n",
      "580/580 [==============================] - 45s 73ms/step - loss: 0.1531 - accuracy: 0.9427 - val_loss: 0.2049 - val_accuracy: 0.9248\n",
      "Epoch 11/15\n",
      "580/580 [==============================] - 47s 76ms/step - loss: 0.1482 - accuracy: 0.9427 - val_loss: 0.1827 - val_accuracy: 0.9321\n",
      "Epoch 12/15\n",
      "580/580 [==============================] - 47s 75ms/step - loss: 0.1460 - accuracy: 0.9454 - val_loss: 0.1835 - val_accuracy: 0.9356\n",
      "Epoch 13/15\n",
      "580/580 [==============================] - 46s 73ms/step - loss: 0.1364 - accuracy: 0.9472 - val_loss: 0.1839 - val_accuracy: 0.9341\n",
      "Epoch 14/15\n",
      "580/580 [==============================] - 51s 75ms/step - loss: 0.1250 - accuracy: 0.9518 - val_loss: 0.1976 - val_accuracy: 0.9248\n",
      "Epoch 15/15\n",
      "580/580 [==============================] - 46s 74ms/step - loss: 0.1257 - accuracy: 0.9509 - val_loss: 0.1947 - val_accuracy: 0.9269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbbb618c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = assign_custom_weights_to_Conv2D_layer(model)\n",
    "    \n",
    "optimizer = SGD(learning_rate=0.001438, momentum=0.767624)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "checkpoint_path = \"tmp/checkpoint_two_inputs_GPU\"\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "model.fit(train_images_gen, batch_size=32, epochs=15, verbose=1, validation_data=test_images_gen, callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.load_weights(checkpoint_path)\n",
    "model.save(\"models/ConvNN_two_inputs_GPU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"models/ConvNN_two_inputs_GPU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 6s 24ms/step - loss: 0.1835 - accuracy: 0.9356\n",
      "Loss: 0.18349914252758026\n",
      "Accuracy: 0.9356217980384827\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(test_images_gen)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 6s 23ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49      3900\n",
      "           1       0.50      0.50      0.50      4053\n",
      "\n",
      "    accuracy                           0.50      7953\n",
      "   macro avg       0.50      0.50      0.50      7953\n",
      "weighted avg       0.50      0.50      0.50      7953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# recall, precision, f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(test_images_gen)\n",
    "threshold = 0.5\n",
    "y_pred = np.where(y_pred > threshold, 1, 0)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2966"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source_image (InputLayer)      [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 8)  208         ['source_image[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 256, 9)  0           ['conv2d[0][0]',                 \n",
      "                                                                  'source_image[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 86, 86, 128)  28928       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 86, 86, 128)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 43, 43, 128)  0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 236672)       0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           3786768     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           544         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,816,481\n",
      "Trainable params: 3,816,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Input for the source image\n",
    "source_input = Input(shape=(256, 256, 1), name=\"source_image\")\n",
    "\n",
    "# First Conv2D layer\n",
    "x = Conv2D(8, (5, 5), activation=\"relu\", padding=\"same\", strides=(1, 1), use_bias=True)(source_input)\n",
    "\n",
    "# Merge the resized source image with the resized output of the first layer\n",
    "merged = Concatenate(axis=-1)([x, source_input])\n",
    "\n",
    "# Second Conv2D layer\n",
    "x = Conv2D(128, (5, 5), activation=\"relu\", padding=\"same\", strides=(3, 3), use_bias=True)(merged)\n",
    "x = Dropout(0.6)(x)\n",
    "x = MaxPooling2D((2, 2), strides=2, padding='valid')(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation=\"relu\", use_bias=True)(x)\n",
    "x = Dense(32, activation=\"relu\", use_bias=True)(x)\n",
    "output = Dense(1, activation=\"sigmoid\", use_bias=True)(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=source_input, outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_with_custom_weights = model.layers[1]\n",
    "model_weights = conv_layer_with_custom_weights.get_weights()[0]\n",
    "for x in range(5):\n",
    "    for y in range(5):\n",
    "        model_weights[x,y,0,0] = (4-x)/4\n",
    "        model_weights[y,x,0,1] = (4-x)/4\n",
    "        model_weights[x,y,0,2] = (x)/4\n",
    "        model_weights[y,x,0,3] = (x)/4\n",
    "        # corner gradients\n",
    "        color = -(4-x)*(4-y)/16 if x > 1 or y > 1 else -0.6\n",
    "        model_weights[x,y,0,4] = color\n",
    "        model_weights[4-y,4-x,0,5] = color\n",
    "        model_weights[4-y,x,0,6] = color\n",
    "        model_weights[y,4-x,0,7] = color\n",
    "# apply the custom weights to the convolutional layer\n",
    "model.layers[1].set_weights([model_weights, np.zeros(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the weights of the convolutional layer\n",
    "weights = model.layers[1].get_weights()[0]\n",
    "\n",
    "# Plot the weights\n",
    "fig, axes = plt.subplots(1, 8, figsize=(20, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(weights[:, :, 0, i], cmap=\"gray\")\n",
    "    ax.set_title(f'Filter {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "572/572 [==============================] - 46s 79ms/step - loss: 0.7903 - accuracy: 0.6227 - val_loss: 0.5674 - val_accuracy: 0.7619\n",
      "Epoch 2/15\n",
      "572/572 [==============================] - 46s 80ms/step - loss: 0.6519 - accuracy: 0.6873 - val_loss: 0.6514 - val_accuracy: 0.5685\n",
      "Epoch 3/15\n",
      "572/572 [==============================] - 46s 80ms/step - loss: 0.5302 - accuracy: 0.7508 - val_loss: 0.3850 - val_accuracy: 0.8445\n",
      "Epoch 4/15\n",
      "572/572 [==============================] - 46s 80ms/step - loss: 0.3798 - accuracy: 0.8354 - val_loss: 0.3316 - val_accuracy: 0.8688\n",
      "Epoch 5/15\n",
      "572/572 [==============================] - 46s 80ms/step - loss: 0.3439 - accuracy: 0.8523 - val_loss: 0.4568 - val_accuracy: 0.7829\n",
      "Epoch 6/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.3977 - accuracy: 0.8168 - val_loss: 0.2990 - val_accuracy: 0.8861\n",
      "Epoch 7/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.2500 - accuracy: 0.9050 - val_loss: 0.2618 - val_accuracy: 0.9010\n",
      "Epoch 8/15\n",
      "572/572 [==============================] - 47s 81ms/step - loss: 0.3006 - accuracy: 0.8721 - val_loss: 0.2376 - val_accuracy: 0.9097\n",
      "Epoch 9/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.1993 - accuracy: 0.9271 - val_loss: 0.2868 - val_accuracy: 0.8857\n",
      "Epoch 10/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.1671 - accuracy: 0.9368 - val_loss: 0.1861 - val_accuracy: 0.9352\n",
      "Epoch 11/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.1518 - accuracy: 0.9434 - val_loss: 0.1728 - val_accuracy: 0.9363\n",
      "Epoch 12/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.1301 - accuracy: 0.9529 - val_loss: 0.1796 - val_accuracy: 0.9352\n",
      "Epoch 13/15\n",
      "572/572 [==============================] - 46s 81ms/step - loss: 0.1128 - accuracy: 0.9590 - val_loss: 0.1436 - val_accuracy: 0.9485\n",
      "Epoch 14/15\n",
      "572/572 [==============================] - 47s 81ms/step - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.1674 - val_accuracy: 0.9389\n",
      "Epoch 15/15\n",
      "572/572 [==============================] - 47s 81ms/step - loss: 0.0872 - accuracy: 0.9692 - val_loss: 0.1610 - val_accuracy: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x186d9e82e00>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.1)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint_path = \"tmp/checkpoint_two_inputs_GPU\"\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "model.fit(train_images_gen, batch_size=16, epochs=15, verbose=1, validation_data=test_images_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"models/model_two_inputs_GPU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"models/model_two_inputs_GPU.h5\")\n",
    "# checkpoint_path = \"tmp/checkpoint_two_inputs_GPU\"\n",
    "# model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_new = np.array([preproces(f'./scraped_images/{path}') for path in os.listdir(\"scraped_images\")])/255.0\n",
    "testY_new = [0 for _ in range(len(testX_new))]\n",
    "testX_new = np.concatenate((testX_new, np.array([preproces(f'./test_set_cropped/{path}') for path in os.listdir(\"test_set_cropped\")])/255.0))\n",
    "testY_new = np.concatenate((testY_new, [1 for _ in range(len(testX_new)-len(testY_new))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_names = np.array([path for path in os.listdir(\"scraped_images\")])\n",
    "testX_names = np.concatenate((testX_names, np.array([path for path in os.listdir(\"test_set_cropped\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 25ms/step - loss: 0.1796 - accuracy: 0.9277\n",
      "19/19 [==============================] - 0s 22ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.59      0.66        69\n",
      "           1       0.95      0.97      0.96       526\n",
      "\n",
      "    accuracy                           0.93       595\n",
      "   macro avg       0.84      0.78      0.81       595\n",
      "weighted avg       0.92      0.93      0.92       595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model and optimize the threshold\n",
    "model.evaluate(testX_new, testY_new)\n",
    "predictions = model.predict(testX_new)\n",
    "threshold = 0.5\n",
    "predictions = (predictions > threshold).astype(int)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testY_new, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(testX_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((595,), (595, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_new.shape, prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions0 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m predictions1 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mprediction\u001b[49m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#if not testY[i]:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m testY_new[i]:\n\u001b[0;32m      6\u001b[0m         predictions0\u001b[38;5;241m.\u001b[39mappend(pred[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "predictions0 = []\n",
    "predictions1 = []\n",
    "for i, pred in enumerate(prediction):\n",
    "    #if not testY[i]:\n",
    "    if not testY_new[i]:\n",
    "        predictions0.append(pred[0])\n",
    "    else:\n",
    "        predictions1.append(pred[0])\n",
    "\n",
    "print (len(predictions0), len(predictions1))\n",
    "try:\n",
    "    print(\"Pred0\")\n",
    "    plt.boxplot(predictions0, positions=[0], widths=0.6)\n",
    "    print(sum(1 for var in predictions0 if var < 0.5))\n",
    "    print(100-round(sum(1 for var in predictions0 if var < 0.5)/len(predictions1)*100,2), \"% of the images are classified as not cropped\")\n",
    "    # print(sum(1 for var in predictions0 if var >= 0.5))\n",
    "    # print(100-round(sum(1 for var in predictions0 if var >= 0.5)/len(predictions0)*100,2), \"% of the images are classified as not cropped\")\n",
    "    print(\"Pred1\")\n",
    "    plt.boxplot(predictions1, positions=[1], widths=0.6)\n",
    "    print(sum(1 for var in predictions1 if var < 0.5))\n",
    "    print(100-round(sum(1 for var in predictions1 if var < 0.5)/len(predictions1)*100,2), \"% of the images are classified as cropped\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "plt.xticks(range(1), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_nested_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# GPU\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 5\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\n\u001b[0;32m      6\u001b[0m conv1_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\n\u001b[0;32m      7\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m Concatenate()([conv1_output, input_image])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py:321\u001b[0m, in \u001b[0;36mFunctional.input\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a layer.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    Only applicable if the layer has exactly one input,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m      AttributeError: If no inbound nodes are found.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nested_inputs\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_nested_inputs'"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), activation=\"relu\", input_shape=(256, 256, 1), padding=\"same\", strides=(3, 3), use_bias=True))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling2D((2, 2), strides=2, padding='valid'))\n",
    "model.add(Conv2D(128, (5, 5), activation=\"relu\", padding=\"same\", strides=(3, 3), use_bias=True))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling2D((2, 2), strides=2, padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation=\"relu\", use_bias=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation=\"relu\", use_bias=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\", use_bias=True))\n",
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.1)\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "#checkpoint_path = \"tmp/checkpoint_20_0.8CDr\"\n",
    "#model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "model.fit(train_images_gen, batch_size=8, epochs=15, verbose=1, validation_data=test_images_gen)#, callbacks=[model_checkpoint])#, callbacks=[reduce_lr])\n",
    "#model.save(\"CNN_model_8_80ep_0.8CDr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.1)\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "#checkpoint_path = \"tmp/checkpoint_20_0.8CDr\"\n",
    "#model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "model.fit(train_images_gen, batch_size=8, epochs=15, verbose=1, validation_data=test_images_gen)#, callbacks=[model_checkpoint])#, callbacks=[reduce_lr])\n",
    "#model.save(\"CNN_model_8_80ep_0.8CDr.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
